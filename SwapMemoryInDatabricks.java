public class SwapMemoryInDatabricks {
    /**
     *  üîπ Databricks Swap Memory - Summary
        1Ô∏è‚É£ What is Swap Memory?
            Swap memory is disk space used as temporary RAM when physical RAM is full.
            OS moves inactive or less-used memory pages from RAM to disk (swap space) to free up RAM.
            Since disk is much slower than RAM, swap usage can slow down performance.
        
        2Ô∏è‚É£ How Swap Works in Databricks?
            Job runs in RAM ‚Äì Data and computations are stored in RAM for fast access.
            RAM fills up ‚Äì If memory demand exceeds available RAM, OS needs more space.
            Swap is used ‚Äì OS moves some data from RAM to disk (swap space).
            CPU accesses swapped data (slow) ‚Äì If the swapped data is needed again, OS moves it back to RAM, causing delays.

        3Ô∏è‚É£ What Does "Swap Memory" Actually Measure?
            Swap memory = Amount of data currently stored on disk due to RAM overflow.
            It does not track the total amount of data moved back and forth.
            Example:
            If 2GB is moved to swap, swap usage = 2GB.
            If 1GB is moved back to RAM, swap usage = 1GB.

        4Ô∏è‚É£ Why is Swap Memory Important in Databricks?
            Databricks handles large datasets in RAM-intensive Spark jobs.
            If RAM runs out, excessive swap usage causes slow performance (disk is slower).
            Monitoring swap helps detect memory bottlenecks and improve efficiency.
           
        5Ô∏è‚É£ How to Reduce Swap Usage in Databricks?
            ‚úÖ Increase RAM ‚Äì More physical memory = Less swap usage.
            ‚úÖ Optimize Queries ‚Äì Avoid loading large datasets into memory all at once.
            ‚úÖ Use Efficient Data Formats ‚Äì Prefer Parquet/ORC over CSV.
            ‚úÖ Tune Spark Configurations ‚Äì Set spark.sql.shuffle.partitions properly.

        üîπ Key Takeaways
            ‚úÖ Swap memory = Data temporarily stored on disk due to RAM overflow.
            ‚úÖ More swap usage = More performance issues (disk is much slower than RAM).
            ‚úÖ Databricks users should monitor and minimize swap usage to avoid slowdowns.
     */

    /*
        üîπ How spark.sql.shuffle.partitions Helps Reduce Swap Memory Usage
        The spark.sql.shuffle.partitions setting controls the number of partitions used during shuffle operations in Spark.

        1Ô∏è‚É£ What Happens When Shuffle Partitions Are Too High?
            Spark splits data into many small partitions for parallel processing.
            Each partition requires memory, and too many partitions can lead to:
                ‚úÖ High memory usage.
                ‚úÖ More spill to disk (swap usage) when RAM is full.
                ‚úÖ Increased shuffle overhead (I/O, CPU).

        2Ô∏è‚É£ What Happens When Shuffle Partitions Are Too Low?
            Few partitions = Less parallelism
            Some executors get overloaded with too much data.
            This can cause out-of-memory (OOM) errors, leading to excessive swap usage when Spark spills data to disk.
        
        3Ô∏è‚É£ How spark.sql.shuffle.partitions Helps?
            Optimizing this setting ensures that shuffle operations use memory efficiently without excessive disk swap or memory overloading.
                ‚úÖ Reducing partitions (if too high) ‚Üí Lowers memory overhead, reducing swap usage.
                ‚úÖ Increasing partitions (if too low) ‚Üí Prevents executors from running out of memory, reducing disk spills.

        4Ô∏è‚É£ Example: Before & After Optimization
            üîπ Before Optimization (spark.sql.shuffle.partitions = 200)
                    200 shuffle partitions ‚Üí Too many small tasks ‚Üí High memory usage ‚Üí Spill to disk (swap).
            üîπ After Optimization (spark.sql.shuffle.partitions = 50)
                    50 shuffle partitions ‚Üí Less memory overhead ‚Üí Less swap usage ‚Üí Faster execution.
        
        üîπ Summary: Why This Matters for Swap Usage in Databricks?
            ‚úÖ Right-sized shuffle partitions ‚Üí Less memory overhead.
            ‚úÖ Less memory overflow ‚Üí Less swap usage (fewer disk spills).
            ‚úÖ Faster execution & better performance.

üí°       Best Practice: Start with spark.sql.shuffle.partitions = num_cores * 2 and adjust based on performance.

    */
}
